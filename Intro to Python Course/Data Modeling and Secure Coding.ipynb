{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14e04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maxwell Tejera, Student #109974513\n",
    "#ISMG 6020 E01\n",
    "#Data modeling and secure coding assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2000137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\tejer\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\tejer\\anaconda3\\lib\\site-packages (from pymongo) (2.4.2)\n"
     ]
    }
   ],
   "source": [
    "#Import anticipated modules\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "!pip install pymongo\n",
    "import pymongo #Get Mongo module going\n",
    "from pymongo import MongoClient #And MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59aaf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make the code more secure, I am adding my Atlas password as a .txt. file instead of in the code directly\n",
    "#I will read my .txt file and concaenate into my uri string:\n",
    "\n",
    "with open(\"C:\\\\Users\\\\tejer\\\\atlas_pw.txt\", 'r') as file:\n",
    "    password = file.read().strip()\n",
    "\n",
    "uri = \"mongodb+srv://tejeramaxwell:\" + password + \"@cluster0.36fc0wp.mongodb.net/?retryWrites=true&w=majority\" \n",
    "#Establish URI to my Atlas\n",
    "\n",
    "#This could be considered my EXPAND UPON criteria to the assignment. It adds more security. But I think it's kind of weak\n",
    "#or just not innovative enough. I want to add another feature at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66e672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 21613\n",
      "Number of columns: 22\n"
     ]
    }
   ],
   "source": [
    "#Now let's define some variables to connect to Atlas / MongoDB\n",
    "\n",
    "client = pymongo.MongoClient(uri)\n",
    "\n",
    "#I previously uploaded a JSON file to Atlas that had data on home sales in King County, WA, in 2014-2015.\n",
    "#It was saved in Atlas with the database as \"kingcounty\" and the collection as \"homesales\"\n",
    "db = client.get_database(\"kingcounty\")\n",
    "collection = db.get_collection(\"homesales\")\n",
    "\n",
    "#Now let's reference the JSON data\n",
    "data = list(collection.find())\n",
    "\n",
    "#I know the data is 21 columns and approximately 20,000 rows. So printing it to check it's referencing correctly\n",
    "#might crash Jupyter. That's preferable as it will require me to be more disciplined with this code!\n",
    "#Let's try to verify the number of rows (documents) and keys (columns) in the JSON data before we start.\n",
    "#Number of rows (documents)\n",
    "num_rows = len(data)\n",
    "\n",
    "#Number of columns (keys across all documents)\n",
    "all_keys = set()  #Using a set to ensure uniqueness\n",
    "for document in data:\n",
    "    all_keys.update(document.keys())\n",
    "num_columns = len(all_keys)\n",
    "\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d781f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns (Unique keys across all documents):\n",
      "sqft_lot15\n",
      "grade\n",
      "bathrooms\n",
      "long\n",
      "yr_built\n",
      "price\n",
      "sqft_living15\n",
      "bedrooms\n",
      "zipcode\n",
      "_id\n",
      "date\n",
      "floors\n",
      "waterfront\n",
      "lat\n",
      "yr_renovated\n",
      "sqft_basement\n",
      "sqft_lot\n",
      "sqft_above\n",
      "sqft_living\n",
      "view\n",
      "id\n",
      "condition\n"
     ]
    }
   ],
   "source": [
    "#Cool, the JSON data is showing what I anticipated, the 21 columns and from the original data the unique Atlas ID.\n",
    "#Let's break that out so I can see the variables in the data and consider which ones are relevant.\n",
    "\n",
    "#Get all unique keys (columns) across all documents\n",
    "all_keys = set()\n",
    "for document in data:\n",
    "    all_keys.update(document.keys())\n",
    "\n",
    "#Print each unique key\n",
    "print(\"Columns (Unique keys across all documents):\")\n",
    "for key in all_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a589f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id: ObjectId\n",
      "id: integer\n",
      "date: string\n",
      "price: float\n",
      "bedrooms: integer\n",
      "bathrooms: float\n",
      "sqft_living: integer\n",
      "sqft_lot: integer\n",
      "floors: float\n",
      "waterfront: integer\n",
      "view: integer\n",
      "condition: integer\n",
      "grade: integer\n",
      "sqft_above: integer\n",
      "sqft_basement: integer\n",
      "yr_built: integer\n",
      "yr_renovated: integer\n",
      "zipcode: integer\n",
      "lat: float\n",
      "long: float\n",
      "sqft_living15: integer\n",
      "sqft_lot15: integer\n"
     ]
    }
   ],
   "source": [
    "#Great, looks like we have just the variables we want for our ~22,000 observations.\n",
    "#Let's validate their type before adding to SQLite.\n",
    "#I want to go through each of the keys in all_keys and determine if they're integers, strings or floats. I know one of\n",
    "#my variables in the dataset is binary. Something to possibly check for later if I want to expand this part of the analysis.\n",
    "\n",
    "#Define a dictionary to store unique types for each key\n",
    "key_types = {}\n",
    "\n",
    "#Define a function to determine value type. JSON data is by default a string so we'll need to convert it to other types.\n",
    "def determine_type(value):\n",
    "    try: #Attempt to validate it's an integer\n",
    "        int(value)\n",
    "        return 'integer'\n",
    "    \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    try: #Attempt to validate it's a float\n",
    "        float(value)\n",
    "        return 'float'\n",
    "    \n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    #If it's neither an integer or float, we conclude it's a string\n",
    "    return 'string'\n",
    "\n",
    "#Now let's run our function on each document (row)\n",
    "for document in data:\n",
    "    for key, value in document.items():\n",
    "        if key not in key_types: #If we haven't determined the data type, set as data type\n",
    "            key_types[key] = set()\n",
    "\n",
    "        if isinstance(value, str): #Check if the value is a string before trying to determine its type\n",
    "            key_types[key].add(determine_type(value))\n",
    "        else:\n",
    "            key_types[key].add(type(value).__name__) #Add float or integer if not string\n",
    "\n",
    "#The for command above was adapted from ChatGPT.\n",
    "#This code will lead to each variable being deemed as a either \"integer\",\"string\", or \"integer, float\" as some true-floats\n",
    "#will have some observations that look like integers. Let's overrite the \"integer, float\" results as \"float\"\n",
    "\n",
    "for key, types in key_types.items():\n",
    "    if \"integer\" in types and \"float\" in types:\n",
    "        key_types[key] = {\"float\"}\n",
    "\n",
    "#Display the results\n",
    "for key, types in key_types.items():\n",
    "    print(f\"{key}: {', '.join(types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ddc1f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal: {'bathrooms', 'yr_built', 'price', 'bedrooms', 'zipcode', 'date', 'floors', 'waterfront', 'yr_renovated', 'sqft_lot', 'sqft_living', 'view', 'id'}\n",
      "After removal: {'bathrooms', 'yr_built', 'price', 'bedrooms', 'zipcode', 'date', 'floors', 'waterfront', 'yr_renovated', 'sqft_lot', 'sqft_living', 'view', 'id'}\n"
     ]
    }
   ],
   "source": [
    "#Cool, the JSON data is showing what I anticipated, the 21 columns and from the original data and the unique Atlas ID.\n",
    "#The _id is the object ID from Atlas. The ID variable is from the original dataset.\n",
    "#I've determined the data is there and what kinds of data are in each variable / column / key.\n",
    "#Let's define the variables I'm interested in.\n",
    "#I am not interested in lat (latitutde), long (longitude), sqft_living15, sqft_lot15, sqft_above, sqft_basement, grade, or\n",
    "#condition. The sqft variables are somewhat redundant. The logitude and latitude won't be relevant in further analysis\n",
    "#as I plan to use zipcode for any mapping exercises.\n",
    "#Grade and condition sound like descriptions of the home quality, but I don't know the details of those criteria so I don't \n",
    "#plan on using in further analysis.\n",
    "#I also want to drop the Atlas ID.\n",
    "\n",
    "#Let's use the .discard command to drop these variables.\n",
    " \n",
    "print(\"Before removal:\", all_keys)\n",
    "keys_to_remove = [\"_id\",\"condition\",\"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\"sqft_above\",\n",
    "                  \"sqft_basement\",\"grade\",\"lat\",\"long\"]\n",
    "for key in keys_to_remove:\n",
    "    all_keys.discard(key)\n",
    "\n",
    "print(\"After removal:\", all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1336af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bathrooms': 'REAL', 'yr_built': 'INTEGER', 'price': 'REAL', 'bedrooms': 'INTEGER', 'zipcode': 'INTEGER', 'date': 'DATE', 'floors': 'REAL', 'waterfront': 'INTEGER', 'yr_renovated': 'INTEGER', 'sqft_lot': 'INTEGER', 'sqft_living': 'INTEGER', 'view': 'INTEGER', 'id': 'INTEGER'}\n"
     ]
    }
   ],
   "source": [
    "#Using our prior results, let's map the JSON keys to SQLite types\n",
    "type_mapping = {\n",
    "    'integer': 'INTEGER',\n",
    "    'float': 'REAL',\n",
    "    'string': 'TEXT'\n",
    "}\n",
    "\n",
    "#Custom mapping for date\n",
    "custom_mappings = {\n",
    "    'date': 'DATE',\n",
    "}\n",
    "\n",
    "#The custom mapping variable seems clumsy, but I struggled to determine if\n",
    "#the variable was datetime based on the string\n",
    "#Known types for each key based on earlier values\n",
    "\n",
    "known_types = {key: list(key_types[key])[0] for key in all_keys if key in key_types}\n",
    "\n",
    "#Map the known types to SQLite types for the keys in all_keys\n",
    "sqlite_types = {key: custom_mappings.get(key, type_mapping[known_types[key]]) for key in all_keys}\n",
    "\n",
    "print(sqlite_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a1a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, I think I have my keys in the right SQLite types.\n",
    "#Let's start talking to SQLite.\n",
    "#Connect to SQLite database\n",
    "conn = sqlite3.connect('C:\\\\Users\\\\tejer\\\\kchomesales.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5a7565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x27252c7b440>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We are connected. Let's write to SQLite.\n",
    "#To do this, we're going to use our JSON keys to execute SQLite CREATE TABLE commands.\n",
    "\n",
    "#Clear data before writing, per the assignment\n",
    "cursor.execute(\"DROP TABLE IF EXISTS kchomesales\")\n",
    "conn.commit()\n",
    "\n",
    "#Define columns using JSON keys\n",
    "table_columns = \", \".join([f\"{key} {value}\" for key, value in sqlite_types.items()])\n",
    "\n",
    "#Create command to be executed in SQLite\n",
    "create_table_command = f\"CREATE TABLE IF NOT EXISTS kchomesales ({table_columns})\"\n",
    "\n",
    "cursor.execute(create_table_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca39ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create an INSERT INTO command variable to run in the SQLite cursor based on my sqlite_types\n",
    "#To keep this secure, we want to add the '?' to prevent someone from running commands on a url or something it thinks is a \n",
    "#string.\n",
    "\n",
    "insert_command = f\"INSERT INTO kchomesales ({', '.join(sqlite_types.keys())}) VALUES ({', '.join(['?' for _ in sqlite_types])})\"\n",
    "\n",
    "#We are almost ready to insert the data, but we need to add a check for missing values.\n",
    "for document in data:\n",
    "    try:\n",
    "        #Extract the values from the JSON document/row based on the JSON keys/column in sqlite_types\n",
    "        values_to_insert = [document[key] for key in sqlite_types.keys()]\n",
    "        \n",
    "        #Execute the insert command\n",
    "        cursor.execute(insert_command, values_to_insert)\n",
    "    \n",
    "    except KeyError as e: #Check for missing values, i.e. if the execute command fails to write a value.\n",
    "        print(f\"Error: Missing field {e} in document {document['id']}\")\n",
    "\n",
    "#Commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a46b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPAND ON THE ASSIGNMENT\n",
    "#I know in future work, I will likely query by price most often. SO let's add an index to that for more optimal memory\n",
    "#usage.\n",
    "\n",
    "cursor.execute(\"CREATE INDEX idx_price ON kchomesales(price)\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "911cd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and columns in SQLite match the expected values!\n"
     ]
    }
   ],
   "source": [
    "#EXPAND ON THE ASSIGNMENT\n",
    "#Also, I want to verify that the data is exactly what I wanted it to be.\n",
    "#Let's run an assertion that confirms the data in my SQLite database is the same dimensions as I wanted.\n",
    "#I.e., the resulting SQLite database has dimensions equal to the original dataset less the exclusions I made during my code\n",
    "#earlier.\n",
    "\n",
    "#Count the rows in SQLite\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM kchomesales\")\n",
    "num_rows_sqlite = cursor.fetchone()[0]\n",
    "\n",
    "#Count the columns in SQLite\n",
    "cursor.execute(\"PRAGMA table_info(kchomesales)\")\n",
    "columns = cursor.fetchall()\n",
    "num_columns_sqlite = (len(columns) - 1) #Need to subtract one because the PRAGMA table indexes at 0\n",
    "\n",
    "#Number of keys/columns removed from consideration\n",
    "removed_keys_count = len(keys_to_remove)\n",
    "\n",
    "#Check if the SQLite data matches the original data less the keys removed.\n",
    "assert num_rows_sqlite == num_rows, f\"Expected {num_rows} rows but got {num_rows_sqlite} rows in SQLite.\"\n",
    "assert num_columns_sqlite == (num_columns - removed_keys_count), f\"Expected {num_columns - removed_keys_count} columns but got {num_columns_sqlite} columns in SQLite.\"\n",
    "\n",
    "print(\"Rows and columns in SQLite match the expected values!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8eac83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36a24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e363732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
